{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus only on common files\n",
    "# fn to compare file name in common_files to section name\n",
    "# if match, add a field to file called match_name and fill it with 1\n",
    "# if not match, add a field to file called match_name and fill it with 0\n",
    "# structure of comparison\n",
    "# {\"section_name\" : {\"match_name\" : 1/0, \"score\" : 0.0}}\n",
    "# score is the score of the file in the section\n",
    "# compare the match_name field between two dicts\n",
    "# if match_name is 1 then compare the score field\n",
    "# if score is equal then add a field called model and fill it with the model name\n",
    "# if score is not equal then add a field called model and fill it with the model name of the file with the highest score\n",
    "# count each of model name (unique values)\n",
    "# the one with higher count is more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(data):\n",
    "    # remove the only_one_model key from each section since its empty anyway\n",
    "    for section in data:\n",
    "        del data[section][\"only_one_model\"]\n",
    "    # convert each list in the common_files key to a key, value pair\n",
    "    for key in data.keys():\n",
    "        data[key] = {file[0]: file[1] for file in data[key][\"common_files\"]}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for section in list of sections\n",
    "# for file name in section\n",
    "# remove suffix \".php\" \n",
    "# if section name == file name\n",
    "# add a field called retrieved and fill it with 1\n",
    "# add a field called score and fill it with the score\n",
    "# if section name != file name\n",
    "# add a field called retrieved and fill it with 0\n",
    "# add a field called score and fill it with the 0.0\n",
    "def create_eval_dict(data):\n",
    "    eval = {}\n",
    "    for section in data:\n",
    "        eval[section] = {}\n",
    "        target_file = section + \".php\"\n",
    "        if target_file in data[section].keys():\n",
    "            eval[section][\"retrieved\"] = 1\n",
    "            eval[section][\"score\"] = data[section][target_file]\n",
    "        else:\n",
    "            eval[section][\"retrieved\"] = 0\n",
    "            eval[section][\"score\"] = 0.0\n",
    "    return eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_df(data):\n",
    "    # convert result dictionary into a dataframe with section names, files names, and scores as columns\n",
    "    data_df = pd.DataFrame([\n",
    "        {\"section_name\": section, \"retrieved\": metrics[\"retrieved\"], \"score\": metrics[\"score\"]}\n",
    "        for section, metrics in data.items()\n",
    "    ])\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieve_accuracy(eval_df):\n",
    "    ones_count = int(eval_df[eval_df[\"retrieved\"] == 1][\"retrieved\"].value_counts().iloc[0])\n",
    "    print(f\"Count of ones: {ones_count}\")\n",
    "    row_count = eval_df.shape[0]\n",
    "    print(f\"Count total observations: {row_count}\")\n",
    "    return ones_count / row_count * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.21739130434783"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_data = read_json_file(\"section_result_azure_embeddings.json\")\n",
    "azure_data = post_process(azure_data)\n",
    "azure_eval = create_eval_dict(azure_data)\n",
    "azure_eval_df = convert_dict_df(azure_eval)\n",
    "azure_retrieve_accuracy = get_retrieve_accuracy(azure_eval_df)\n",
    "azure_retrieve_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.86956521739131"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_data = read_json_file(\"section_result_openai_embeddings.json\")\n",
    "openai_data = post_process(openai_data)\n",
    "openai_eval = create_eval_dict(openai_data)\n",
    "openai_eval_df = convert_dict_df(openai_eval)\n",
    "openai_retrieve_accuracy = get_retrieve_accuracy(openai_eval_df)\n",
    "openai_retrieve_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
