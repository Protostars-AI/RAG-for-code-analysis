{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raoneg/Bumblebee-AI/RAG for code analysis/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/raoneg/Bumblebee-AI/RAG for code analysis/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/raoneg/Bumblebee-AI/RAG for code analysis/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from build_embeddings import build_embeddings, get_file_embeddings\n",
    "from search import get_total_files, query_top_files, query_top_files_specter, get_common_files_with_avg_score, get_unique_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Initialize embeddings and model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))\n",
    "model = SentenceTransformer('sentence-transformers/allenai-specter', device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load OWASP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OWASP data\n",
    "owasp_df = pd.read_csv('OWASP Controls - Application Security.csv')\n",
    "owasp_df = owasp_df[~owasp_df['req_description'].str.contains('\\[DELETED')]\n",
    "owasp_df['req_description'] = owasp_df['req_description'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode JSON object\n",
    "def decode_json_object(array):\n",
    "    files = {}\n",
    "    for file_name, file_content in array.items():\n",
    "        if file_name.endswith(('.py', '.sh', '.java', '.php', '.js', '.htm', '.html', '.vue')):\n",
    "            string_base64 = array[file_name]['content']\n",
    "            decodedBytes = base64.b64decode(string_base64)\n",
    "            files[file_name] = decodedBytes.decode(\"utf-8\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function\n",
    "def background_code_matching(repo_files, repo_id):\n",
    "    try:\n",
    "        repo_id = str(repo_id)\n",
    "        section_result = {}\n",
    "        build_embeddings(repo_files, repo_id)\n",
    "        \n",
    "        for section in owasp_df['section_name'].unique():\n",
    "            reqs_list = list(owasp_df[owasp_df['section_name'] == section]['req_description'])\n",
    "            req_str = ' '.join(reqs_list)\n",
    "            query = req_str\n",
    "            depth = get_total_files(repo_id)\n",
    "            results_ada = query_top_files(query, depth, repo_id)\n",
    "            results_specter = query_top_files_specter(query, depth, repo_id)\n",
    "            \n",
    "            common_files_with_avg_score = get_common_files_with_avg_score(results_ada, results_specter)\n",
    "            unique_model = get_unique_files(results_ada, results_specter)\n",
    "            result_dict = {\n",
    "                'common_files': common_files_with_avg_score,\n",
    "                'only_one_model': unique_model\n",
    "            }\n",
    "            section_result[section] = result_dict\n",
    "        \n",
    "        # Save the section_result dictionary to a .json file\n",
    "        with open(f'section_result_{repo_id}.json', 'w') as json_file:\n",
    "            json.dump(section_result, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Results saved to section_result_{repo_id}.json\")\n",
    "        return section_result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Task failed: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the JSON file\n",
    "def process_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            repo_data = json.load(json_file)\n",
    "\n",
    "        decoded_files = decode_json_object(repo_data)\n",
    "        \n",
    "        # Save decoded content to separate files\n",
    "        output_dir = \"decoded_files\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file_name, content in decoded_files.items():\n",
    "            output_path = os.path.join(output_dir, file_name)\n",
    "            with open(output_path, 'w') as output_file:\n",
    "                output_file.write(content)\n",
    "                logging.info(f\"Decoded content saved to {output_path}\")\n",
    "\n",
    "        return decoded_files\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing JSON file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = \"output 2.json\" \n",
    "decoded_content = process_json_file(json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
